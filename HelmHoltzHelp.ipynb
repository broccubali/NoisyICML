{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XlUO5XuxrTzy",
        "outputId": "b8811e08-2967-4c8d-a575-6e18ad446b21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'NoisyICML' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/broccubali/NoisyICML.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyDOE"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKffXNnexYwL",
        "outputId": "6cdae52b-5467-483f-e212-53e250b765b7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyDOE in /usr/local/lib/python3.11/dist-packages (0.3.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pyDOE) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from pyDOE) (1.13.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from scipy.io import loadmat\n",
        "from tqdm import tqdm\n",
        "from pyDOE import lhs\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "Kd1myEnlw3B5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class PINN(nn.Module):\n",
        "#     def __init__(self, input_size, hidden_size, output_size):\n",
        "#         super(PINN, self).__init__()\n",
        "#         self.layers = nn.ModuleList(\n",
        "#             [\n",
        "#                 nn.Linear(input_size if i == 0 else hidden_size, hidden_size)\n",
        "#                 if i % 2 == 0\n",
        "#                 else nn.Tanh()\n",
        "#                 for i in range(10)\n",
        "#             ]\n",
        "#         )\n",
        "#         self.layers.append(nn.Linear(hidden_size, output_size))\n",
        "\n",
        "#         # Trainable parameter for the wave number squared (k^2)\n",
        "#         self.k2 = nn.Parameter(torch.tensor([30.0], dtype=torch.float32, device=\"cuda\"))\n",
        "\n",
        "#         # Optimizer\n",
        "#         self.optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
        "#         self.optimizer.param_groups[0][\"params\"].append(self.k2)\n",
        "\n",
        "#         self.loss = nn.MSELoss()\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         for layer in self.layers:\n",
        "#             x = layer(x)\n",
        "#         return x\n",
        "\n",
        "#     def loss_fn(self, x, u):\n",
        "#         u_pred = self.forward(x)\n",
        "#         return self.loss(u_pred, u)\n",
        "\n",
        "#     # def residual_loss(self, xtrain):\n",
        "#     #     g = xtrain.clone()\n",
        "#     #     g.requires_grad = True\n",
        "#     #     u_pred = self.forward(g)\n",
        "\n",
        "#     #     # Compute gradients\n",
        "#     #     u_grad = torch.autograd.grad(\n",
        "#     #         u_pred, g, torch.ones_like(u_pred), retain_graph=True, create_graph=True\n",
        "#     #     )[0]\n",
        "#     #     u_lap = torch.autograd.grad(\n",
        "#     #         u_grad, g, torch.ones_like(u_grad), create_graph=True\n",
        "#     #     )[0].sum(dim=1, keepdim=True)\n",
        "\n",
        "#     #     # Residual form of the Helmholtz equation\n",
        "#     #     residual = u_lap + self.k2 * u_pred\n",
        "#     #     return self.loss(residual, torch.zeros_like(residual))\n",
        "\n",
        "#     def residual_loss(self, xtrain, fhat):\n",
        "#         g = xtrain.clone()\n",
        "#         g.requires_grad = True\n",
        "#         u_pred = self.forward(g)\n",
        "\n",
        "#         # Compute gradients\n",
        "#         u_grad = torch.autograd.grad(\n",
        "#             u_pred, g, torch.ones_like(u_pred), create_graph=True, retain_graph=True\n",
        "#         )[0]\n",
        "#         u_xx = torch.autograd.grad(\n",
        "#             u_grad[:, [0]], g, torch.ones_like(u_grad[:, [0]]), create_graph=True\n",
        "#         )[0][:, [0]]\n",
        "#         u_tt = torch.autograd.grad(\n",
        "#             u_grad[:, [1]], g, torch.ones_like(u_grad[:, [1]]), create_graph=True\n",
        "#         )[0][:, [1]]\n",
        "\n",
        "#         # Residual calculation\n",
        "#         residual = u_xx + u_tt + self.k2 * u_pred - fhat\n",
        "#         return torch.mean(residual**2)  # Mean squared error for the residual\n",
        "\n",
        "\n",
        "#     def total_loss(self, xtrain, utrain):\n",
        "#         alpha_female = 10.0\n",
        "#         data_loss = self.loss_fn(xtrain, utrain)  # Match observed data\n",
        "#         physics_loss = self.residual_loss(xtrain)  # Enforce governing equations\n",
        "#         return data_loss + alpha_female* physics_loss\n",
        "\n",
        "#     def train_model(self, xtrain, utrain, epochs=10000):\n",
        "#         for epoch in tqdm(range(epochs)):\n",
        "#             self.optimizer.zero_grad()\n",
        "#             loss = self.total_loss(xtrain, utrain)\n",
        "#             loss.backward()\n",
        "#             self.optimizer.step()\n",
        "\n",
        "#             # Logging\n",
        "#             if epoch % 1000 == 0:\n",
        "#                 print(\n",
        "#                     f\"Epoch {epoch}, Loss {loss.item()}, \"\n",
        "#                     f\"k^2 (Wave Number Squared) {self.k2.item()}\"\n",
        "#                 )"
      ],
      "metadata": {
        "id": "QQcdlIbbxXdS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import Adam\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import torch.nn.init as init\n",
        "\n",
        "class PINN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(PINN, self).__init__()\n",
        "\n",
        "        # Define network layers with alternating Linear and Tanh\n",
        "        self.layers = nn.ModuleList(\n",
        "            [\n",
        "                nn.Linear(input_size if i == 0 else hidden_size, hidden_size)\n",
        "                if i % 2 == 0\n",
        "                else nn.Tanh()\n",
        "                for i in range(20)\n",
        "            ]\n",
        "        )\n",
        "        self.layers.append(nn.Linear(hidden_size, output_size))  # Final output layer\n",
        "\n",
        "        for layer in self.layers:\n",
        "            if isinstance(layer, nn.Linear):\n",
        "                init.xavier_normal_(layer.weight)\n",
        "                if layer.bias is not None:\n",
        "                    init.zeros_(layer.bias)\n",
        "\n",
        "        # Trainable parameter for wave number squared (kÂ²)\n",
        "        self.k2 = nn.Parameter(torch.tensor([25.0], dtype=torch.float32, device=\"cuda\"))\n",
        "\n",
        "        # Loss function\n",
        "        self.loss = nn.MSELoss()\n",
        "\n",
        "        # Optimizer and scheduler\n",
        "        self.physics_optimizer = Adam(self.parameters(), lr=1e-2)\n",
        "        self.loss_optimizer = Adam(self.parameters(), lr=1e-2)\n",
        "        self.physics_optimizer.param_groups[0][\"params\"].append(self.k2)\n",
        "        self.loss_optimizer.param_groups[0][\"params\"].append(self.k2)\n",
        "        self.scheduler_physics = StepLR(self.physics_optimizer, step_size=1000, gamma=0.5)  # Adjusted for frequent updates\n",
        "        self.scheduler_loss = StepLR(self.loss_optimizer, step_size=1000, gamma=0.5)  # Adjusted for frequent updates\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "    def loss_fn(self, x, u):\n",
        "        u_pred = self.forward(x)\n",
        "        return self.loss(u_pred, u)\n",
        "\n",
        "    def residual_loss(self, xtrain):\n",
        "        # Enable gradient computation for second-order derivatives\n",
        "        g = xtrain.clone().requires_grad_(True)\n",
        "        u_pred = self.forward(g)\n",
        "\n",
        "        # first-order derivatives\n",
        "        u_grad = torch.autograd.grad(\n",
        "            u_pred, g, torch.ones_like(u_pred), create_graph=True, retain_graph=True\n",
        "        )[0]\n",
        "\n",
        "        # second-order derivatives\n",
        "        u_xx = torch.autograd.grad(\n",
        "            u_grad[:, 0], g, torch.ones_like(u_grad[:, 0]), create_graph=True, retain_graph=True\n",
        "        )[0][:, [0]]\n",
        "        u_tt = torch.autograd.grad(\n",
        "            u_grad[:, 1], g, torch.ones_like(u_grad[:, 1]), create_graph=True, retain_graph=True\n",
        "        )[0][:, [1]]\n",
        "\n",
        "        # helmholtz residual\n",
        "        fhat = -self.k2 * torch.sin(torch.sqrt(self.k2) * g[:,[0]]) * torch.sin(torch.sqrt(self.k2) * g[:, [1]])\n",
        "        # print(fhat)\n",
        "        residual = u_xx + u_tt + self.k2 * u_pred - fhat\n",
        "        return torch.mean(residual**2)\n",
        "\n",
        "    def total_loss(self, xtrain, utrain):\n",
        "        data_loss = self.loss_fn(xtrain, utrain)\n",
        "        physics_loss = self.residual_loss(xtrain)\n",
        "        # print(\"data loss\", data_loss, \"physics loss\", physics_loss)\n",
        "        # Reduced physics loss weight for better stability\n",
        "        return data_loss + 100 * physics_loss\n",
        "\n",
        "    def train_model(self, xtrain, utrain, epochs=10000):\n",
        "        for epoch in tqdm(range(epochs)):\n",
        "            self.loss_optimizer.zero_grad()\n",
        "            self.physics_optimizer.zero_grad()\n",
        "            loss = self.loss_fn(xtrain, utrain)\n",
        "            # print(\"data loss\", loss.item())\n",
        "            loss.backward()\n",
        "            self.loss_optimizer.step()\n",
        "            loss = self.residual_loss(xtrain)\n",
        "            # print(\"physics loss\", loss.item())\n",
        "            loss.backward()\n",
        "            self.physics_optimizer.step()\n",
        "            # Gradient clipping to stabilize training\n",
        "            # torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=1.0)\n",
        "            self.physics_optimizer.step()\n",
        "            self.loss_optimizer.step()\n",
        "            self.scheduler_physics.step()\n",
        "            self.scheduler_loss.step()\n",
        "            # self.scheduler.step()\n",
        "\n",
        "            if epoch % 1000 == 0:\n",
        "                print(f\"Epoch {epoch}, Loss: {loss.item():.6f}, kÂ²: {self.k2.item():.6f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "t2-HYwAmyFV7"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "u = np.load(\"/content/NoisyICML/pinns-inverse/Helmholtz/helmholtz_solution.npy\") # BOB THE BUILDER\n",
        "x = np.load(\"/content/NoisyICML/pinns-inverse/Helmholtz/x_coordinate.npy\")  # BOB THE BUILDER\n",
        "t = np.load(\"/content/NoisyICML/pinns-inverse/Helmholtz/t_coordinate.npy\")"
      ],
      "metadata": {
        "id": "yiS_fH8yyGoQ"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(x, dtype=torch.float32)\n",
        "t = torch.tensor(t, dtype=torch.float32)\n",
        "u = torch.tensor(u, dtype=torch.float32).T"
      ],
      "metadata": {
        "id": "vgJBOyBCyRwR"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, T = np.meshgrid(x, t)\n",
        "xtrue = np.hstack((X.flatten()[:, None], T.flatten()[:, None]))\n",
        "utrue = u.flatten()[:, None]"
      ],
      "metadata": {
        "id": "OPn_ovAvyX9E"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape, T.shape, u.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2EPajR_4Xfy3",
        "outputId": "9e5332e4-182c-476b-f0ae-277ce3f83a61"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((100, 256), (100, 256), torch.Size([100, 256]))"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx = np.random.choice(xtrue.shape[0], 10000, replace=False)\n",
        "xtrain = xtrue[idx, :]\n",
        "utrain = u.flatten()[idx][:, None]"
      ],
      "metadata": {
        "id": "uICpD_g1yYoe"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "utrain.shape, xtrain.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRO4tb77Y1Bu",
        "outputId": "8eceabee-e627-42cb-d2a4-40024fcf79e4"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([10000, 1]), (10000, 2))"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\")\n",
        "Xtrain = torch.tensor(xtrain, dtype=torch.float32, device=device)\n",
        "Xtrue = torch.tensor(xtrue, dtype=torch.float32, device=device)\n",
        "Utrain = utrain.to(device)"
      ],
      "metadata": {
        "id": "CrBsklrRym0K"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = PINN(input_size=2, hidden_size=30, output_size=1).to(device)\n",
        "model.train_model(Xtrain, Utrain, epochs=10000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "JutNa7BeydGE",
        "outputId": "e4eb9eb1-ada6-4863-a6bb-fa3f9fec762b"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 11/10000 [00:00<03:18, 50.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 474.864899, kÂ²: 24.977310\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|â         | 1008/10000 [00:22<03:21, 44.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1000, Loss: 21.519346, kÂ²: 8.801414\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 20%|ââ        | 2009/10000 [00:43<02:41, 49.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2000, Loss: 0.272653, kÂ²: 1.451031\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 30%|âââ       | 3009/10000 [01:05<02:19, 49.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3000, Loss: 0.014951, kÂ²: 0.672559\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 40%|ââââ      | 4007/10000 [01:27<02:11, 45.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4000, Loss: 0.003180, kÂ²: 0.480523\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|âââââ     | 5006/10000 [01:48<02:01, 41.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5000, Loss: 0.001198, kÂ²: 0.392909\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 60%|ââââââ    | 6006/10000 [02:10<01:19, 50.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6000, Loss: 0.000606, kÂ²: 0.335920\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 70%|âââââââ   | 7005/10000 [02:32<01:01, 48.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7000, Loss: 0.000348, kÂ²: 0.293149\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 79%|ââââââââ  | 7897/10000 [02:51<00:45, 46.09it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-1995536221a1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPINN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-38-9f0bd212ef7f>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self, xtrain, utrain, epochs)\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;31m# print(\"data loss\", loss.item())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresidual_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1ptmfSQpzRBM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}