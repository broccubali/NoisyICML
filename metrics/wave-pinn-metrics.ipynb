{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class PINN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, c):\n",
    "        super(PINN, self).__init__()\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                (\n",
    "                    nn.Linear(input_size if i == 0 else hidden_size, hidden_size)\n",
    "                    if i % 2 == 0\n",
    "                    else nn.Tanh()\n",
    "                )\n",
    "                for i in range(20)\n",
    "            ]\n",
    "        )\n",
    "        self.layers.append(nn.Linear(hidden_size, output_size))\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.c = c\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def loss_fn(self, x, u):\n",
    "        u_pred = self.forward(x)\n",
    "        return self.loss(u_pred, u)\n",
    "\n",
    "    def residual_loss(self, xtrain, fhat):\n",
    "        g = xtrain.clone()\n",
    "        g.requires_grad = True\n",
    "\n",
    "        u_pred = self.forward(g) # now I predict u(x,t)\n",
    "\n",
    "        u_x_t = torch.autograd.grad(\n",
    "            u_pred,\n",
    "            g,\n",
    "            torch.ones([xtrain.shape[0], 1]).to(\"cuda\"),\n",
    "            retain_graph=True,\n",
    "            create_graph=True,\n",
    "        )[0] # first derivatives wrt x and t\n",
    "\n",
    "        u_xx_tt = torch.autograd.grad(\n",
    "            u_x_t, g, torch.ones(xtrain.shape).to(\"cuda\"), create_graph=True\n",
    "        )[0] # second derivatives wrt x and t\n",
    "\n",
    "        # u_x = u_x_t[:, [0]]\n",
    "        # u_t = u_x_t[:, [1]]\n",
    "        u_xx = u_xx_tt[:, [0]] # second derivative wrt x\n",
    "        u_tt = u_xx_tt[:, [1]] # second derivative wrt t\n",
    "\n",
    "        # residual = u_tt - c^2 * u_xx\n",
    "        residual = u_tt - self.c ** 2 * u_xx\n",
    "        return self.loss(residual, fhat)        \n",
    "\n",
    "    def total_loss(self, xtrain, utrain, fhat):\n",
    "        return self.loss_fn(xtrain, utrain) + self.residual_loss(xtrain, fhat)\n",
    "\n",
    "    def train_model(self, xtrain, utrain, epochs=1000):\n",
    "        fhat = torch.zeros(xtrain.shape[0], 1, device=\"cuda\")\n",
    "        for epoch in range(epochs):\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = self.total_loss(xtrain, utrain, fhat)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            if epoch % 1000 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss {loss.item()}\")\n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1024,), (201,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.linspace(-1, 1, 1024)\n",
    "t = np.linspace(0, 2, 201)\n",
    "x.shape, t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, T = np.meshgrid(x, t)\n",
    "xtrue = np.hstack((X.flatten()[:, None], T.flatten()[:, None]))\n",
    "xtrue = torch.tensor(xtrue, dtype=torch.float32, device=\"cuda\")\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def loadAndPrep(u):\n",
    "    idx = np.random.choice(u.flatten().shape[0], 10000, replace=False)\n",
    "    xtrain = xtrue[idx, :]\n",
    "    utrain = u.flatten()[idx][:, None]\n",
    "    utrain = torch.tensor(utrain, dtype=torch.float32).to(device)\n",
    "    return xtrain, utrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "\n",
    "def trainAndLog(u, e):\n",
    "    xtrain, utrain = loadAndPrep(u)\n",
    "    model = PINN(input_size=2, hidden_size=20, output_size=1, c=e).to(\"cuda\")\n",
    "    loss = model.train_model(xtrain, utrain, epochs=5000)\n",
    "    with torch.no_grad():\n",
    "        pred = model(xtrue).cpu().numpy()\n",
    "    l = np.mean((u.flatten() - pred.flatten()) ** 2)\n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    return l, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Epoch 0, Loss 3.13653564453125\n",
      "Epoch 1000, Loss 0.041664618998765945\n",
      "Epoch 2000, Loss 0.04165833443403244\n",
      "Epoch 3000, Loss 0.041656751185655594\n",
      "Epoch 4000, Loss 0.0416557602584362\n",
      "Epoch 0, Loss 5.3877482414245605\n",
      "Epoch 1000, Loss 0.06837137043476105\n",
      "Epoch 2000, Loss 0.06836983561515808\n",
      "Epoch 3000, Loss 0.06836891919374466\n",
      "Epoch 4000, Loss 0.0683683305978775\n",
      "1\n",
      "Epoch 0, Loss 0.7186174988746643\n",
      "Epoch 1000, Loss 0.08626752346754074\n",
      "Epoch 2000, Loss 0.08571188151836395\n",
      "Epoch 3000, Loss 0.08291883766651154\n",
      "Epoch 4000, Loss 0.08452849090099335\n",
      "Epoch 0, Loss 0.41061967611312866\n",
      "Epoch 1000, Loss 0.1177259013056755\n",
      "Epoch 2000, Loss 0.1129646822810173\n",
      "Epoch 3000, Loss 0.11431840062141418\n",
      "Epoch 4000, Loss 0.11309686303138733\n",
      "2\n",
      "Epoch 0, Loss 0.0832967683672905\n",
      "Epoch 1000, Loss 0.03883598744869232\n",
      "Epoch 2000, Loss 0.07810428738594055\n",
      "Epoch 3000, Loss 0.027922095730900764\n",
      "Epoch 4000, Loss 0.02752327360212803\n",
      "Epoch 0, Loss 0.14124156534671783\n",
      "Epoch 1000, Loss 0.055902544409036636\n",
      "Epoch 2000, Loss 0.052882492542266846\n",
      "Epoch 3000, Loss 0.05187565088272095\n",
      "Epoch 4000, Loss 0.05076051130890846\n",
      "3\n",
      "Epoch 0, Loss 0.18700362741947174\n",
      "Epoch 1000, Loss 0.03699924051761627\n",
      "Epoch 2000, Loss 0.02718295343220234\n",
      "Epoch 3000, Loss 0.019757745787501335\n",
      "Epoch 4000, Loss 0.026122625917196274\n",
      "Epoch 0, Loss 0.30822718143463135\n",
      "Epoch 1000, Loss 0.06846205145120621\n",
      "Epoch 2000, Loss 0.0686563029885292\n",
      "Epoch 3000, Loss 0.0684918612241745\n",
      "Epoch 4000, Loss 0.053490668535232544\n",
      "4\n",
      "Epoch 0, Loss 0.04044513776898384\n",
      "Epoch 1000, Loss 0.027128886431455612\n",
      "Epoch 2000, Loss 0.021497000008821487\n",
      "Epoch 3000, Loss 0.020417919382452965\n",
      "Epoch 4000, Loss 0.020035646855831146\n",
      "Epoch 0, Loss 0.17175351083278656\n",
      "Epoch 1000, Loss 0.062200043350458145\n",
      "Epoch 2000, Loss 0.062495823949575424\n",
      "Epoch 3000, Loss 0.04822733253240585\n",
      "Epoch 4000, Loss 0.04685423523187637\n",
      "5\n",
      "Epoch 0, Loss 0.058608945459127426\n",
      "Epoch 1000, Loss 0.021273914724588394\n",
      "Epoch 2000, Loss 0.019337713718414307\n",
      "Epoch 3000, Loss 0.018949858844280243\n",
      "Epoch 4000, Loss 0.019256003201007843\n",
      "Epoch 0, Loss 0.08057869970798492\n",
      "Epoch 1000, Loss 0.045077770948410034\n",
      "Epoch 2000, Loss 0.045019522309303284\n",
      "Epoch 3000, Loss 0.04508433863520622\n",
      "Epoch 4000, Loss 0.04496714845299721\n",
      "6\n",
      "Epoch 0, Loss 0.06838799268007278\n",
      "Epoch 1000, Loss 0.025517594069242477\n",
      "Epoch 2000, Loss 0.02225065417587757\n",
      "Epoch 3000, Loss 0.02080126665532589\n",
      "Epoch 4000, Loss 0.020016495138406754\n",
      "Epoch 0, Loss 0.08003657311201096\n",
      "Epoch 1000, Loss 0.05384249612689018\n",
      "Epoch 2000, Loss 0.056584689766168594\n",
      "Epoch 3000, Loss 0.054644741117954254\n",
      "Epoch 4000, Loss 0.051601242274045944\n",
      "7\n",
      "Epoch 0, Loss 0.045272666960954666\n",
      "Epoch 1000, Loss 0.042447518557310104\n",
      "Epoch 2000, Loss 0.03490421921014786\n",
      "Epoch 3000, Loss 0.03177836537361145\n",
      "Epoch 4000, Loss 0.02597072906792164\n",
      "Epoch 0, Loss 0.1003209799528122\n",
      "Epoch 1000, Loss 0.051643725484609604\n",
      "Epoch 2000, Loss 0.047647494822740555\n",
      "Epoch 3000, Loss 0.045655976980924606\n",
      "Epoch 4000, Loss 0.045183323323726654\n",
      "8\n",
      "Epoch 0, Loss 0.055163003504276276\n",
      "Epoch 1000, Loss 0.039445649832487106\n",
      "Epoch 2000, Loss 0.03949667513370514\n",
      "Epoch 3000, Loss 0.03949286416172981\n",
      "Epoch 4000, Loss 0.03937355428934097\n",
      "Epoch 0, Loss 0.13173379004001617\n",
      "Epoch 1000, Loss 0.0674484595656395\n",
      "Epoch 2000, Loss 0.06667383760213852\n",
      "Epoch 3000, Loss 0.06742282211780548\n",
      "Epoch 4000, Loss 0.06733458489179611\n",
      "9\n",
      "Epoch 0, Loss 0.295245498418808\n",
      "Epoch 1000, Loss 0.046363621950149536\n",
      "Epoch 2000, Loss 0.04636107757687569\n",
      "Epoch 3000, Loss 0.04635841026902199\n",
      "Epoch 4000, Loss 0.04635296389460564\n",
      "Epoch 0, Loss 2.7860209941864014\n",
      "Epoch 1000, Loss 0.07153552025556564\n",
      "Epoch 2000, Loss 0.07153031975030899\n",
      "Epoch 3000, Loss 0.07152914255857468\n",
      "Epoch 4000, Loss 0.07152851670980453\n",
      "10\n",
      "Epoch 0, Loss 0.22558985650539398\n",
      "Epoch 1000, Loss 0.03972696512937546\n",
      "Epoch 2000, Loss 0.03972664102911949\n",
      "Epoch 3000, Loss 0.039726439863443375\n",
      "Epoch 4000, Loss 0.03972615301609039\n",
      "Epoch 0, Loss 21.328872680664062\n",
      "Epoch 1000, Loss 0.06745890527963638\n",
      "Epoch 2000, Loss 0.0674193724989891\n",
      "Epoch 3000, Loss 0.06741486489772797\n",
      "Epoch 4000, Loss 0.06741401553153992\n",
      "11\n",
      "Epoch 0, Loss 0.18487714231014252\n",
      "Epoch 1000, Loss 0.05056175962090492\n",
      "Epoch 2000, Loss 0.05055103078484535\n",
      "Epoch 3000, Loss 0.05060555785894394\n",
      "Epoch 4000, Loss 0.050433121621608734\n",
      "Epoch 0, Loss 0.41657865047454834\n",
      "Epoch 1000, Loss 0.07689570635557175\n",
      "Epoch 2000, Loss 0.07686623930931091\n",
      "Epoch 3000, Loss 0.07688255608081818\n",
      "Epoch 4000, Loss 0.07689410448074341\n",
      "12\n",
      "Epoch 0, Loss 4.976966857910156\n",
      "Epoch 1000, Loss 0.04001527279615402\n",
      "Epoch 2000, Loss 0.040013909339904785\n",
      "Epoch 3000, Loss 0.04001369699835777\n",
      "Epoch 4000, Loss 0.04001360386610031\n",
      "Epoch 0, Loss 2.768976926803589\n",
      "Epoch 1000, Loss 0.06888271123170853\n",
      "Epoch 2000, Loss 0.06887750327587128\n",
      "Epoch 3000, Loss 0.06887569278478622\n",
      "Epoch 4000, Loss 0.06887510418891907\n",
      "13\n",
      "Epoch 0, Loss 0.6623728275299072\n",
      "Epoch 1000, Loss 0.0861748680472374\n",
      "Epoch 2000, Loss 0.08615510165691376\n",
      "Epoch 3000, Loss 0.08618263155221939\n",
      "Epoch 4000, Loss 0.08630240708589554\n",
      "Epoch 0, Loss 1.3588454723358154\n",
      "Epoch 1000, Loss 0.11420655995607376\n",
      "Epoch 2000, Loss 0.11419618129730225\n",
      "Epoch 3000, Loss 0.11418721079826355\n",
      "Epoch 4000, Loss 0.11418318003416061\n",
      "14\n",
      "Epoch 0, Loss 0.2763531804084778\n",
      "Epoch 1000, Loss 0.02981976978480816\n",
      "Epoch 2000, Loss 0.029817283153533936\n",
      "Epoch 3000, Loss 0.029807742685079575\n",
      "Epoch 4000, Loss 0.02979188784956932\n",
      "Epoch 0, Loss 0.3021916449069977\n",
      "Epoch 1000, Loss 0.05704888328909874\n",
      "Epoch 2000, Loss 0.05704852193593979\n",
      "Epoch 3000, Loss 0.057048238813877106\n",
      "Epoch 4000, Loss 0.05704788491129875\n",
      "15\n",
      "Epoch 0, Loss 1.2672446966171265\n",
      "Epoch 1000, Loss 0.08577580749988556\n",
      "Epoch 2000, Loss 0.08576975017786026\n",
      "Epoch 3000, Loss 0.08576763421297073\n",
      "Epoch 4000, Loss 0.08576636761426926\n",
      "Epoch 0, Loss 2.1282129287719727\n",
      "Epoch 1000, Loss 0.11425409466028214\n",
      "Epoch 2000, Loss 0.11425259709358215\n",
      "Epoch 3000, Loss 0.1142517477273941\n",
      "Epoch 4000, Loss 0.1142512857913971\n",
      "16\n",
      "Epoch 0, Loss 0.13267815113067627\n",
      "Epoch 1000, Loss 0.043310653418302536\n",
      "Epoch 2000, Loss 0.04330206289887428\n",
      "Epoch 3000, Loss 0.04331352189183235\n",
      "Epoch 4000, Loss 0.04331047832965851\n",
      "Epoch 0, Loss 0.16079631447792053\n",
      "Epoch 1000, Loss 0.0696173906326294\n",
      "Epoch 2000, Loss 0.06965327262878418\n",
      "Epoch 3000, Loss 0.06961026042699814\n",
      "Epoch 4000, Loss 0.06963063031435013\n",
      "17\n",
      "Epoch 0, Loss 0.09038485586643219\n",
      "Epoch 1000, Loss 0.040797386318445206\n",
      "Epoch 2000, Loss 0.0408366359770298\n",
      "Epoch 3000, Loss 0.040537700057029724\n",
      "Epoch 4000, Loss 0.04085002839565277\n",
      "Epoch 0, Loss 0.3086912930011749\n",
      "Epoch 1000, Loss 0.06573478132486343\n",
      "Epoch 2000, Loss 0.06554131209850311\n",
      "Epoch 3000, Loss 0.06566527485847473\n",
      "Epoch 4000, Loss 0.06560876965522766\n",
      "18\n",
      "Epoch 0, Loss 0.06099127605557442\n",
      "Epoch 1000, Loss 0.04969487339258194\n",
      "Epoch 2000, Loss 0.05735033005475998\n",
      "Epoch 3000, Loss 0.057388123124837875\n",
      "Epoch 4000, Loss 0.052126217633485794\n",
      "Epoch 0, Loss 0.23652741312980652\n",
      "Epoch 1000, Loss 0.08371514827013016\n",
      "Epoch 2000, Loss 0.06928367167711258\n",
      "Epoch 3000, Loss 0.08436878025531769\n",
      "Epoch 4000, Loss 0.08006708323955536\n",
      "19\n",
      "Epoch 0, Loss 1.2159730195999146\n",
      "Epoch 1000, Loss 0.05931348726153374\n",
      "Epoch 2000, Loss 0.05931048095226288\n",
      "Epoch 3000, Loss 0.05930883437395096\n",
      "Epoch 4000, Loss 0.05930691584944725\n",
      "Epoch 0, Loss 1.235479474067688\n",
      "Epoch 1000, Loss 0.08451438695192337\n",
      "Epoch 2000, Loss 0.08451293408870697\n",
      "Epoch 3000, Loss 0.0845123752951622\n",
      "Epoch 4000, Loss 0.08451187610626221\n",
      "20\n",
      "Epoch 0, Loss 3.7049195766448975\n",
      "Epoch 1000, Loss 0.03264767676591873\n",
      "Epoch 2000, Loss 0.03262310475111008\n",
      "Epoch 3000, Loss 0.03261630982160568\n",
      "Epoch 4000, Loss 0.03261372447013855\n",
      "Epoch 0, Loss 1.9260857105255127\n",
      "Epoch 1000, Loss 0.06041239947080612\n",
      "Epoch 2000, Loss 0.0604078434407711\n",
      "Epoch 3000, Loss 0.06040532514452934\n",
      "Epoch 4000, Loss 0.06040389463305473\n",
      "21\n",
      "Epoch 0, Loss 22.20824432373047\n",
      "Epoch 1000, Loss 0.03694175183773041\n",
      "Epoch 2000, Loss 0.036922045052051544\n",
      "Epoch 3000, Loss 0.03691629692912102\n",
      "Epoch 4000, Loss 0.03691348806023598\n",
      "Epoch 0, Loss 8.69980525970459\n",
      "Epoch 1000, Loss 0.06572295725345612\n",
      "Epoch 2000, Loss 0.06564007699489594\n",
      "Epoch 3000, Loss 0.06562448292970657\n",
      "Epoch 4000, Loss 0.06562270224094391\n",
      "22\n",
      "Epoch 0, Loss 0.04068494960665703\n",
      "Epoch 1000, Loss 0.030239038169384003\n",
      "Epoch 2000, Loss 0.03023427166044712\n",
      "Epoch 3000, Loss 0.030240273103117943\n",
      "Epoch 4000, Loss 0.030205518007278442\n",
      "Epoch 0, Loss 0.138569638133049\n",
      "Epoch 1000, Loss 0.056386012583971024\n",
      "Epoch 2000, Loss 0.05638134106993675\n",
      "Epoch 3000, Loss 0.056368302553892136\n",
      "Epoch 4000, Loss 0.05637895315885544\n",
      "23\n",
      "Epoch 0, Loss 2.0763888359069824\n",
      "Epoch 1000, Loss 0.04082002118229866\n",
      "Epoch 2000, Loss 0.040817003697156906\n",
      "Epoch 3000, Loss 0.04081664979457855\n",
      "Epoch 4000, Loss 0.04081651195883751\n",
      "Epoch 0, Loss 0.8696774840354919\n",
      "Epoch 1000, Loss 0.06580665707588196\n",
      "Epoch 2000, Loss 0.06580577045679092\n",
      "Epoch 3000, Loss 0.06580530107021332\n",
      "Epoch 4000, Loss 0.06580488383769989\n",
      "24\n",
      "Epoch 0, Loss 8.411852836608887\n",
      "Epoch 1000, Loss 0.030361853539943695\n",
      "Epoch 2000, Loss 0.030359189957380295\n",
      "Epoch 3000, Loss 0.030358178541064262\n",
      "Epoch 4000, Loss 0.030357563868165016\n",
      "Epoch 0, Loss 4.118737697601318\n",
      "Epoch 1000, Loss 0.05758406221866608\n",
      "Epoch 2000, Loss 0.05756503343582153\n",
      "Epoch 3000, Loss 0.057558052241802216\n",
      "Epoch 4000, Loss 0.057555001229047775\n",
      "25\n",
      "Epoch 0, Loss 3.561434507369995\n",
      "Epoch 1000, Loss 0.037824951112270355\n",
      "Epoch 2000, Loss 0.03782378137111664\n",
      "Epoch 3000, Loss 0.037823569029569626\n",
      "Epoch 4000, Loss 0.03782337158918381\n",
      "Epoch 0, Loss 4.37263822555542\n",
      "Epoch 1000, Loss 0.06430993229150772\n",
      "Epoch 2000, Loss 0.06430665403604507\n",
      "Epoch 3000, Loss 0.06430495530366898\n",
      "Epoch 4000, Loss 0.06430400907993317\n",
      "26\n",
      "Epoch 0, Loss 0.727083146572113\n",
      "Epoch 1000, Loss 0.08640363067388535\n",
      "Epoch 2000, Loss 0.08640111237764359\n",
      "Epoch 3000, Loss 0.086395263671875\n",
      "Epoch 4000, Loss 0.08649493753910065\n",
      "Epoch 0, Loss 1.6004395484924316\n",
      "Epoch 1000, Loss 0.11450488865375519\n",
      "Epoch 2000, Loss 0.1144460141658783\n",
      "Epoch 3000, Loss 0.11440927535295486\n",
      "Epoch 4000, Loss 0.1142272874712944\n",
      "27\n",
      "Epoch 0, Loss 0.03496506065130234\n",
      "Epoch 1000, Loss 0.03007103130221367\n",
      "Epoch 2000, Loss 0.03006112389266491\n",
      "Epoch 3000, Loss 0.03005700372159481\n",
      "Epoch 4000, Loss 0.030031511560082436\n",
      "Epoch 0, Loss 0.08184698224067688\n",
      "Epoch 1000, Loss 0.059016283601522446\n",
      "Epoch 2000, Loss 0.05974791198968887\n",
      "Epoch 3000, Loss 0.059722527861595154\n",
      "Epoch 4000, Loss 0.05974801257252693\n",
      "28\n",
      "Epoch 0, Loss 0.09937907755374908\n",
      "Epoch 1000, Loss 0.06155999004840851\n",
      "Epoch 2000, Loss 0.06154965981841087\n",
      "Epoch 3000, Loss 0.06155873090028763\n",
      "Epoch 4000, Loss 0.06155369430780411\n",
      "Epoch 0, Loss 4.755749225616455\n",
      "Epoch 1000, Loss 0.08988761901855469\n",
      "Epoch 2000, Loss 0.0898805484175682\n",
      "Epoch 3000, Loss 0.08987732231616974\n",
      "Epoch 4000, Loss 0.08987537771463394\n",
      "29\n",
      "Epoch 0, Loss 12.64667797088623\n",
      "Epoch 1000, Loss 0.04016006737947464\n",
      "Epoch 2000, Loss 0.040141113102436066\n",
      "Epoch 3000, Loss 0.04013875126838684\n",
      "Epoch 4000, Loss 0.04013692960143089\n",
      "Epoch 0, Loss 12.466734886169434\n",
      "Epoch 1000, Loss 0.06636407971382141\n",
      "Epoch 2000, Loss 0.0663522332906723\n",
      "Epoch 3000, Loss 0.06635064631700516\n",
      "Epoch 4000, Loss 0.06634989380836487\n",
      "30\n",
      "Epoch 0, Loss 15.69002914428711\n",
      "Epoch 1000, Loss 0.03930769860744476\n",
      "Epoch 2000, Loss 0.03929969295859337\n",
      "Epoch 3000, Loss 0.039297107607126236\n",
      "Epoch 4000, Loss 0.0392954982817173\n",
      "Epoch 0, Loss 1.0219671726226807\n",
      "Epoch 1000, Loss 0.06705239415168762\n",
      "Epoch 2000, Loss 0.06704457104206085\n",
      "Epoch 3000, Loss 0.06704380363225937\n",
      "Epoch 4000, Loss 0.06704366952180862\n",
      "31\n",
      "Epoch 0, Loss 0.07330235093832016\n",
      "Epoch 1000, Loss 0.045949943363666534\n",
      "Epoch 2000, Loss 0.046370264142751694\n",
      "Epoch 3000, Loss 0.04611664265394211\n",
      "Epoch 4000, Loss 0.04605960473418236\n",
      "Epoch 0, Loss 0.08032380044460297\n",
      "Epoch 1000, Loss 0.0750029906630516\n",
      "Epoch 2000, Loss 0.07501457631587982\n",
      "Epoch 3000, Loss 0.0749039575457573\n",
      "Epoch 4000, Loss 0.07481672614812851\n",
      "32\n",
      "Epoch 0, Loss 2.1881191730499268\n",
      "Epoch 1000, Loss 0.05674346908926964\n",
      "Epoch 2000, Loss 0.05672738328576088\n",
      "Epoch 3000, Loss 0.05672457814216614\n",
      "Epoch 4000, Loss 0.056723110377788544\n",
      "Epoch 0, Loss 5.431380748748779\n",
      "Epoch 1000, Loss 0.08665069192647934\n",
      "Epoch 2000, Loss 0.08664485067129135\n",
      "Epoch 3000, Loss 0.0866423025727272\n",
      "Epoch 4000, Loss 0.08664122968912125\n",
      "33\n",
      "Epoch 0, Loss 0.10959210991859436\n",
      "Epoch 1000, Loss 0.04442409798502922\n",
      "Epoch 2000, Loss 0.04442380368709564\n",
      "Epoch 3000, Loss 0.04442078620195389\n",
      "Epoch 4000, Loss 0.04439207911491394\n",
      "Epoch 0, Loss 0.12906873226165771\n",
      "Epoch 1000, Loss 0.07113569229841232\n",
      "Epoch 2000, Loss 0.07111211121082306\n",
      "Epoch 3000, Loss 0.07110509276390076\n",
      "Epoch 4000, Loss 0.07112003862857819\n",
      "34\n",
      "Epoch 0, Loss 0.1572856456041336\n",
      "Epoch 1000, Loss 0.007801123894751072\n",
      "Epoch 2000, Loss 0.007738273125141859\n",
      "Epoch 3000, Loss 0.006807464640587568\n",
      "Epoch 4000, Loss 0.00653016148135066\n",
      "Epoch 0, Loss 0.3447036147117615\n",
      "Epoch 1000, Loss 0.034686796367168427\n",
      "Epoch 2000, Loss 0.034616611897945404\n",
      "Epoch 3000, Loss 0.03427518904209137\n",
      "Epoch 4000, Loss 0.03467455133795738\n",
      "35\n",
      "Epoch 0, Loss 0.37577855587005615\n",
      "Epoch 1000, Loss 0.03079562447965145\n",
      "Epoch 2000, Loss 0.03079436719417572\n",
      "Epoch 3000, Loss 0.030793439596891403\n",
      "Epoch 4000, Loss 0.030791347846388817\n",
      "Epoch 0, Loss 0.6303080916404724\n",
      "Epoch 1000, Loss 0.05706506967544556\n",
      "Epoch 2000, Loss 0.057063158601522446\n",
      "Epoch 3000, Loss 0.05706286057829857\n",
      "Epoch 4000, Loss 0.05706271901726723\n",
      "36\n",
      "Epoch 0, Loss 0.812652587890625\n",
      "Epoch 1000, Loss 0.03340398520231247\n",
      "Epoch 2000, Loss 0.03339815139770508\n",
      "Epoch 3000, Loss 0.03339608758687973\n",
      "Epoch 4000, Loss 0.033402688801288605\n",
      "Epoch 0, Loss 1.0660113096237183\n",
      "Epoch 1000, Loss 0.05994166433811188\n",
      "Epoch 2000, Loss 0.05994074046611786\n",
      "Epoch 3000, Loss 0.05994050204753876\n",
      "Epoch 4000, Loss 0.05994030833244324\n",
      "37\n",
      "Epoch 0, Loss 9.096104621887207\n",
      "Epoch 1000, Loss 0.07975408434867859\n",
      "Epoch 2000, Loss 0.07974886149168015\n",
      "Epoch 3000, Loss 0.07974813878536224\n",
      "Epoch 4000, Loss 0.0797475278377533\n",
      "Epoch 0, Loss 1.140976071357727\n",
      "Epoch 1000, Loss 0.10661468654870987\n",
      "Epoch 2000, Loss 0.10661201179027557\n",
      "Epoch 3000, Loss 0.10660897940397263\n",
      "Epoch 4000, Loss 0.10660981386899948\n",
      "38\n",
      "Epoch 0, Loss 1.6151505708694458\n",
      "Epoch 1000, Loss 0.03686070814728737\n",
      "Epoch 2000, Loss 0.03683514520525932\n",
      "Epoch 3000, Loss 0.03682970628142357\n",
      "Epoch 4000, Loss 0.036828067153692245\n",
      "Epoch 0, Loss 0.24211850762367249\n",
      "Epoch 1000, Loss 0.0639585629105568\n",
      "Epoch 2000, Loss 0.06395630538463593\n",
      "Epoch 3000, Loss 0.06395583599805832\n",
      "Epoch 4000, Loss 0.06395558267831802\n",
      "39\n",
      "Epoch 0, Loss 0.932831883430481\n",
      "Epoch 1000, Loss 0.06752628087997437\n",
      "Epoch 2000, Loss 0.0675206258893013\n",
      "Epoch 3000, Loss 0.06751849502325058\n",
      "Epoch 4000, Loss 0.06751751899719238\n",
      "Epoch 0, Loss 1.6563208103179932\n",
      "Epoch 1000, Loss 0.09819663316011429\n",
      "Epoch 2000, Loss 0.09819463640451431\n",
      "Epoch 3000, Loss 0.09819335490465164\n",
      "Epoch 4000, Loss 0.09819074720144272\n",
      "40\n",
      "Epoch 0, Loss 0.7588501572608948\n",
      "Epoch 1000, Loss 0.051312122493982315\n",
      "Epoch 2000, Loss 0.05130818113684654\n",
      "Epoch 3000, Loss 0.0512968972325325\n",
      "Epoch 4000, Loss 0.05128064379096031\n",
      "Epoch 0, Loss 1.9277509450912476\n",
      "Epoch 1000, Loss 0.0772169902920723\n",
      "Epoch 2000, Loss 0.07721468061208725\n",
      "Epoch 3000, Loss 0.07721351087093353\n",
      "Epoch 4000, Loss 0.0772179365158081\n",
      "41\n",
      "Epoch 0, Loss 9.541145324707031\n",
      "Epoch 1000, Loss 0.03303869068622589\n",
      "Epoch 2000, Loss 0.03301481157541275\n",
      "Epoch 3000, Loss 0.03300626948475838\n",
      "Epoch 4000, Loss 0.033003855496644974\n",
      "Epoch 0, Loss 1.581714153289795\n",
      "Epoch 1000, Loss 0.060874320566654205\n",
      "Epoch 2000, Loss 0.0608702227473259\n",
      "Epoch 3000, Loss 0.06086840108036995\n",
      "Epoch 4000, Loss 0.0608673170208931\n",
      "42\n",
      "Epoch 0, Loss 0.08720023185014725\n",
      "Epoch 1000, Loss 0.03440595418214798\n",
      "Epoch 2000, Loss 0.034393467009067535\n",
      "Epoch 3000, Loss 0.03440525010228157\n",
      "Epoch 4000, Loss 0.034413788467645645\n",
      "Epoch 0, Loss 0.2244732528924942\n",
      "Epoch 1000, Loss 0.06337887793779373\n",
      "Epoch 2000, Loss 0.06366565823554993\n",
      "Epoch 3000, Loss 0.06337623298168182\n",
      "Epoch 4000, Loss 0.06337988376617432\n",
      "43\n",
      "Epoch 0, Loss 1.3435652256011963\n",
      "Epoch 1000, Loss 0.03243446350097656\n",
      "Epoch 2000, Loss 0.032433610409498215\n",
      "Epoch 3000, Loss 0.03243253007531166\n",
      "Epoch 4000, Loss 0.032430216670036316\n",
      "Epoch 0, Loss 0.6248365640640259\n",
      "Epoch 1000, Loss 0.059581395238637924\n",
      "Epoch 2000, Loss 0.05957864969968796\n",
      "Epoch 3000, Loss 0.05957745760679245\n",
      "Epoch 4000, Loss 0.059576742351055145\n",
      "44\n",
      "Epoch 0, Loss 0.13652420043945312\n",
      "Epoch 1000, Loss 0.032170988619327545\n",
      "Epoch 2000, Loss 0.032140038907527924\n",
      "Epoch 3000, Loss 0.03217480331659317\n",
      "Epoch 4000, Loss 0.03234267979860306\n",
      "Epoch 0, Loss 0.07650567591190338\n",
      "Epoch 1000, Loss 0.05933883413672447\n",
      "Epoch 2000, Loss 0.05929854139685631\n",
      "Epoch 3000, Loss 0.05937562510371208\n",
      "Epoch 4000, Loss 0.059292133897542953\n",
      "45\n",
      "Epoch 0, Loss 4.944151401519775\n",
      "Epoch 1000, Loss 0.04721381142735481\n",
      "Epoch 2000, Loss 0.04720788076519966\n",
      "Epoch 3000, Loss 0.04720557853579521\n",
      "Epoch 4000, Loss 0.047204598784446716\n",
      "Epoch 0, Loss 0.5008748173713684\n",
      "Epoch 1000, Loss 0.07751026749610901\n",
      "Epoch 2000, Loss 0.0775095522403717\n",
      "Epoch 3000, Loss 0.07750928401947021\n",
      "Epoch 4000, Loss 0.0775090754032135\n",
      "46\n",
      "Epoch 0, Loss 1.9402334690093994\n",
      "Epoch 1000, Loss 0.06875786185264587\n",
      "Epoch 2000, Loss 0.06875498592853546\n",
      "Epoch 3000, Loss 0.06875342130661011\n",
      "Epoch 4000, Loss 0.06880101561546326\n",
      "Epoch 0, Loss 5.709189414978027\n",
      "Epoch 1000, Loss 0.09802432358264923\n",
      "Epoch 2000, Loss 0.09802231192588806\n",
      "Epoch 3000, Loss 0.09802103042602539\n",
      "Epoch 4000, Loss 0.0980198010802269\n",
      "47\n",
      "Epoch 0, Loss 12.458270072937012\n",
      "Epoch 1000, Loss 0.03365093097090721\n",
      "Epoch 2000, Loss 0.033637359738349915\n",
      "Epoch 3000, Loss 0.03363369032740593\n",
      "Epoch 4000, Loss 0.03363100439310074\n",
      "Epoch 0, Loss 38.05186462402344\n",
      "Epoch 1000, Loss 0.062297213822603226\n",
      "Epoch 2000, Loss 0.06218299642205238\n",
      "Epoch 3000, Loss 0.062168434262275696\n",
      "Epoch 4000, Loss 0.06216610223054886\n",
      "48\n",
      "Epoch 0, Loss 0.04432443156838417\n",
      "Epoch 1000, Loss 0.01996774598956108\n",
      "Epoch 2000, Loss 0.01887643337249756\n",
      "Epoch 3000, Loss 0.018498126417398453\n",
      "Epoch 4000, Loss 0.018273446708917618\n",
      "Epoch 0, Loss 0.09653349220752716\n",
      "Epoch 1000, Loss 0.045405153185129166\n",
      "Epoch 2000, Loss 0.0450005941092968\n",
      "Epoch 3000, Loss 0.044988200068473816\n",
      "Epoch 4000, Loss 0.04500669986009598\n",
      "49\n",
      "Epoch 0, Loss 0.5476145148277283\n",
      "Epoch 1000, Loss 0.040129534900188446\n",
      "Epoch 2000, Loss 0.040127359330654144\n",
      "Epoch 3000, Loss 0.04011626914143562\n",
      "Epoch 4000, Loss 0.03998022899031639\n",
      "Epoch 0, Loss 0.17456158995628357\n",
      "Epoch 1000, Loss 0.07060269266366959\n",
      "Epoch 2000, Loss 0.07111487537622452\n",
      "Epoch 3000, Loss 0.07112035155296326\n",
      "Epoch 4000, Loss 0.07061192393302917\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import random\n",
    "import json\n",
    "\n",
    "random.seed(69)\n",
    "d = {}\n",
    "with h5py.File(\"data1.h5\", \"r\") as f:\n",
    "    a = random.choices(list(f.keys()), k=50)\n",
    "    n = 0\n",
    "    for i in a:\n",
    "        print(n)\n",
    "        d[i] = []\n",
    "        e = f[i][\"c\"][()]\n",
    "        uclean = f[i][\"clean\"][:]\n",
    "        mse, loss = trainAndLog(uclean, e)\n",
    "        d[i].append({\"clean\": [{\"mse\": float(mse), \"loss\": float(loss)}]})\n",
    "        unoisy = f[i][\"noisy\"][:]\n",
    "        mse, loss = trainAndLog(unoisy, e)\n",
    "        d[i].append({\"noisy\": [{\"mse\": float(mse), \"loss\": float(loss)}]})\n",
    "        n += 1\n",
    "        with open(\"results.json\", \"w\") as g:\n",
    "            json.dump(d, g)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
