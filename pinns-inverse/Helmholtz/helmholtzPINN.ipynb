{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from tqdm import tqdm\n",
    "from pyDOE import lhs\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(PINN, self).__init__()\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                nn.Linear(input_size if i == 0 else hidden_size, hidden_size)\n",
    "                if i % 2 == 0\n",
    "                else nn.Tanh()\n",
    "                for i in range(10)\n",
    "            ]\n",
    "        )\n",
    "        self.layers.append(nn.Linear(hidden_size, output_size))\n",
    "        \n",
    "        # Trainable parameter for the wave number squared (k^2)\n",
    "        self.k2 = nn.Parameter(torch.tensor([30.0], dtype=torch.float32, device=\"cuda\"))\n",
    "\n",
    "        # Optimizer\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        self.optimizer.param_groups[0][\"params\"].append(self.k2)\n",
    "        \n",
    "        self.loss = nn.MSELoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def loss_fn(self, x, u):\n",
    "        u_pred = self.forward(x)\n",
    "        return self.loss(u_pred, u)\n",
    "\n",
    "    # def residual_loss(self, xtrain):\n",
    "    #     g = xtrain.clone()\n",
    "    #     g.requires_grad = True\n",
    "    #     u_pred = self.forward(g)\n",
    "        \n",
    "    #     # Compute gradients\n",
    "    #     u_grad = torch.autograd.grad(\n",
    "    #         u_pred, g, torch.ones_like(u_pred), retain_graph=True, create_graph=True\n",
    "    #     )[0]\n",
    "    #     u_lap = torch.autograd.grad(\n",
    "    #         u_grad, g, torch.ones_like(u_grad), create_graph=True\n",
    "    #     )[0].sum(dim=1, keepdim=True)\n",
    "        \n",
    "    #     # Residual form of the Helmholtz equation\n",
    "    #     residual = u_lap + self.k2 * u_pred\n",
    "    #     return self.loss(residual, torch.zeros_like(residual))\n",
    "    #     THIS IS VERY STUPID OMFG LET'S TRY SOMETHING ELSE CUZ THE GODS HATE ME AND LIFE IS AN ENDLESS PIT OF DESPAIR THAT IS VOID OF ALL HAPPINESS\n",
    "    \n",
    "    def residual_loss(self, xtrain, fhat):\n",
    "        g = xtrain.clone()\n",
    "        g.requires_grad = True\n",
    "        u_pred = self.forward(g)\n",
    "        \n",
    "        # Compute gradients\n",
    "        u_grad = torch.autograd.grad(\n",
    "            u_pred, g, torch.ones_like(u_pred), create_graph=True, retain_graph=True\n",
    "        )[0]\n",
    "        u_xx = torch.autograd.grad(\n",
    "            u_grad[:, [0]], g, torch.ones_like(u_grad[:, [0]]), create_graph=True\n",
    "        )[0][:, [0]]\n",
    "        u_tt = torch.autograd.grad(\n",
    "            u_grad[:, [1]], g, torch.ones_like(u_grad[:, [1]]), create_graph=True\n",
    "        )[0][:, [1]]\n",
    "        \n",
    "        # Residual calculation\n",
    "        residual = u_xx + u_tt + self.k2 * u_pred - fhat\n",
    "        return torch.mean(residual**2)  # Mean squared error for the residual\n",
    "\n",
    "\n",
    "    def total_loss(self, xtrain, utrain):\n",
    "        alpha_female = 10.0\n",
    "        data_loss = self.loss_fn(xtrain, utrain)  # Match observed data\n",
    "        physics_loss = self.residual_loss(xtrain)  # Enforce governing equations\n",
    "        return data_loss + alpha_female* physics_loss\n",
    "\n",
    "    def train_model(self, xtrain, utrain, epochs=10000):\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = self.total_loss(xtrain, utrain)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            # Logging\n",
    "            if epoch % 1000 == 0:\n",
    "                print(\n",
    "                    f\"Epoch {epoch}, Loss {loss.item()}, \"\n",
    "                    f\"k^2 (Wave Number Squared) {self.k2.item()}\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# again.\n",
    "\n",
    "class PINN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(PINN, self).__init__()\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                nn.Linear(input_size if i == 0 else hidden_size, hidden_size)\n",
    "                if i % 2 == 0\n",
    "                else nn.Tanh()\n",
    "                for i in range(10)\n",
    "            ]\n",
    "        )\n",
    "        self.layers.append(nn.Linear(hidden_size, output_size))\n",
    "        \n",
    "        # Trainable parameter for the wave number squared (k^2)\n",
    "        self.k2 = nn.Parameter(torch.tensor([1.0], dtype=torch.float32)) \n",
    "\n",
    "        \n",
    "\n",
    "        # Optimizer\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=1e-2)  # Larger learning rate\n",
    "        # self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=1000, gamma=0.9)  # Scheduler to reduce LR\n",
    "\n",
    "        self.loss = nn.MSELoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def loss_fn(self, x, u):\n",
    "        u_pred = self.forward(x)\n",
    "        return self.loss(u_pred, u)\n",
    "\n",
    "    def residual_loss(self, xtrain, fhat):\n",
    "        # Physics-informed loss based on the Helmholtz equation\n",
    "        g = xtrain.clone()\n",
    "        g.requires_grad = True\n",
    "        u_pred = self.forward(g)\n",
    "        \n",
    "        # Compute the gradients for second derivatives (u_xx and u_tt)\n",
    "        u_grad = torch.autograd.grad(\n",
    "            u_pred, g, torch.ones_like(u_pred), create_graph=True, retain_graph=True\n",
    "        )[0]\n",
    "        u_xx = torch.autograd.grad(\n",
    "            u_grad[:, [0]], g, torch.ones_like(u_grad[:, [0]]), create_graph=True\n",
    "        )[0][:, [0]]\n",
    "        u_tt = torch.autograd.grad(\n",
    "            u_grad[:, [1]], g, torch.ones_like(u_grad[:, [1]]), create_graph=True\n",
    "        )[0][:, [1]]\n",
    "        \n",
    "        # Residual calculation for Helmholtz equation: u_xx + u_tt + k^2 * u = fhat\n",
    "        residual = u_xx + u_tt + self.k2 * u_pred - fhat\n",
    "        return self.loss(residual, fhat)\n",
    "\n",
    "    def total_loss(self, xtrain, utrain):\n",
    "        fhat = torch.zeros(xtrain.shape[0], 1)\n",
    "        alpha_female = 10.0\n",
    "        data_loss = self.loss_fn(xtrain, utrain)  # Match observed data\n",
    "        physics_loss = self.residual_loss(xtrain, fhat)  # Enforce governing equations\n",
    "        return data_loss + alpha_female * physics_loss\n",
    "\n",
    "    def train_model(self, xtrain, utrain, epochs=10000):\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = self.total_loss(xtrain, utrain)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            # self.scheduler.step()\n",
    "            \n",
    "            # Logging\n",
    "            if epoch % 1000 == 0:\n",
    "                print(\n",
    "                    f\"Epoch {epoch}, Loss {loss.item()}, \"\n",
    "                    f\"k^2 (Wave Number Squared) {self.k2.item()}\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = np.load(\"C:/Users/ASUS/NoisyICML/pinns-inverse/Helmholtz/helmholtz_solution.npy\")  # BOB THE BUILDER\n",
    "x = np.load(\"C:/Users/ASUS/NoisyICML/pinns-inverse/Helmholtz/x_coordinate.npy\")  # BOB THE BUILDER\n",
    "t = np.load(\"C:/Users/ASUS/NoisyICML/pinns-inverse/Helmholtz/t_coordinate.npy\")[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(x, dtype=torch.float32)\n",
    "t = torch.tensor(t, dtype=torch.float32)\n",
    "u = torch.tensor(u, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, T = np.meshgrid(x, t)\n",
    "xtrue = np.hstack((X.flatten()[:, None], T.flatten()[:, None]))\n",
    "utrue = u.flatten()[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.1+cpu\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)  # Check PyTorch version\n",
    "print(torch.cuda.is_available())  # Check if CUDA is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_19528\\369146865.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Utrain = torch.tensor(utrain, dtype=torch.float32)\n",
      "  0%|          | 3/20000 [00:00<22:16, 14.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss 0.6556071043014526, k^2 (Wave Number Squared) 0.9900000095367432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 666/20000 [00:48<23:18, 13.83it/s]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m Utrain \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(utrain, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m      8\u001b[0m model \u001b[38;5;241m=\u001b[39m PINN(input_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, hidden_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, output_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mUtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[12], line 68\u001b[0m, in \u001b[0;36mPINN.train_model\u001b[1;34m(self, xtrain, utrain, epochs)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     67\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_loss(xtrain, utrain)\n\u001b[1;32m---> 68\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# self.scheduler.step()\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# Logging\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "idx = np.random.choice(xtrue.shape[0], 10000, replace=False)\n",
    "xtrain = xtrue[idx, :]\n",
    "utrain = utrue[idx, :]\n",
    "\n",
    "Xtrain = torch.tensor(xtrain, dtype=torch.float32)\n",
    "Utrain = torch.tensor(utrain, dtype=torch.float32)\n",
    "\n",
    "model = PINN(input_size=2, hidden_size=20, output_size=1)\n",
    "model.train_model(Xtrain, Utrain, epochs=20000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 39/10000 [00:00<00:49, 199.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss 0.24674396216869354, k^2 (Wave Number Squared) -3.5944936275482178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1037/10000 [00:04<00:38, 231.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000, Loss 0.24692048132419586, k^2 (Wave Number Squared) -3.796750545501709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2028/10000 [00:09<00:37, 211.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2000, Loss 0.24673013389110565, k^2 (Wave Number Squared) -3.9797747135162354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3034/10000 [00:14<00:33, 206.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3000, Loss 0.24671880900859833, k^2 (Wave Number Squared) -4.151811599731445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4033/10000 [00:19<00:31, 190.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4000, Loss 0.24678117036819458, k^2 (Wave Number Squared) -4.319501876831055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5034/10000 [00:24<00:26, 184.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5000, Loss 0.24664579331874847, k^2 (Wave Number Squared) -4.480327129364014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6025/10000 [00:29<00:19, 208.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6000, Loss 0.24662648141384125, k^2 (Wave Number Squared) -4.639685153961182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7019/10000 [00:35<00:15, 189.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7000, Loss 0.24660000205039978, k^2 (Wave Number Squared) -4.799276828765869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8025/10000 [00:40<00:10, 196.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8000, Loss 0.24660082161426544, k^2 (Wave Number Squared) -4.954023361206055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9026/10000 [00:45<00:05, 192.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9000, Loss 0.2465580701828003, k^2 (Wave Number Squared) -5.104205131530762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:50<00:00, 197.15it/s]\n"
     ]
    }
   ],
   "source": [
    "model.train_model(Xtrain, Utrain, epochs=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 44/20000 [00:00<01:30, 220.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss 0.24655897915363312, k^2 (Wave Number Squared) -5.256725311279297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1034/20000 [00:04<01:43, 183.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000, Loss 0.24651500582695007, k^2 (Wave Number Squared) -5.409544944763184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2033/20000 [00:10<01:32, 194.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2000, Loss 0.2464950978755951, k^2 (Wave Number Squared) -5.551908016204834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3036/20000 [00:15<01:25, 198.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3000, Loss 0.24648025631904602, k^2 (Wave Number Squared) -5.694790363311768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4035/20000 [00:20<01:27, 182.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4000, Loss 0.24646392464637756, k^2 (Wave Number Squared) -5.843469142913818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5023/20000 [00:25<01:18, 190.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5000, Loss 0.24643580615520477, k^2 (Wave Number Squared) -6.004410743713379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6022/20000 [00:31<01:08, 203.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6000, Loss 0.24642540514469147, k^2 (Wave Number Squared) -6.164248943328857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7039/20000 [00:37<01:07, 192.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7000, Loss 0.24643132090568542, k^2 (Wave Number Squared) -6.322986125946045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 8029/20000 [00:42<01:04, 186.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8000, Loss 0.24638040363788605, k^2 (Wave Number Squared) -6.473191261291504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 9023/20000 [00:48<00:58, 186.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9000, Loss 0.2463667392730713, k^2 (Wave Number Squared) -6.613042831420898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10024/20000 [00:54<01:02, 158.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10000, Loss 0.2463499754667282, k^2 (Wave Number Squared) -6.742490768432617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11027/20000 [00:59<00:45, 195.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11000, Loss 0.24633604288101196, k^2 (Wave Number Squared) -6.8663010597229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 12027/20000 [01:04<00:37, 212.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12000, Loss 0.2463223785161972, k^2 (Wave Number Squared) -6.982793807983398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 13033/20000 [01:09<00:32, 214.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13000, Loss 0.24634069204330444, k^2 (Wave Number Squared) -7.089256286621094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 14018/20000 [01:15<00:39, 153.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14000, Loss 0.24629852175712585, k^2 (Wave Number Squared) -7.184641361236572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 15030/20000 [01:21<00:26, 184.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15000, Loss 0.24628783762454987, k^2 (Wave Number Squared) -7.271185398101807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 16031/20000 [01:26<00:20, 195.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16000, Loss 0.2462777942419052, k^2 (Wave Number Squared) -7.350324630737305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 17039/20000 [01:30<00:13, 226.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17000, Loss 0.24644015729427338, k^2 (Wave Number Squared) -7.422645568847656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 18035/20000 [01:36<00:09, 196.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18000, Loss 0.24625959992408752, k^2 (Wave Number Squared) -7.487943649291992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 19041/20000 [01:40<00:04, 224.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19000, Loss 0.2462511956691742, k^2 (Wave Number Squared) -7.54771089553833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [01:45<00:00, 188.79it/s]\n"
     ]
    }
   ],
   "source": [
    "model.train_model(Xtrain, Utrain, epochs=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
